
## Permissive Information-Flow Analysis for Large Language Models

In this paper, we propose a novel approach to **propagate more permissive information-flow labels** in LLM-based applications which we will refer to as an influence-based label propagator (LP). The key idea of our approach is to **propagate only the labels of the samples that were influential in generating the model’s output**—and drop the labels of the inputs that are not required. Specifically, for a given context and fixed tolerance λ, we identify all subcontexts for which the model achieves utility that is at most λ below the utility of the full context. Within those subcontexts, we then select the one with the most permissive label. We prove that, under idealizing assumptions, our algorithm identifies the most permissive label(s) possible. An example of our approach is shown in Figure 1.

To ground our work in a relevant LLM system, we implement and evaluate two different realizations of our label propagator: (i) a RAG-based system [19] where the retrieved documents are provided within the prompt to an autoregressive LLM, and (ii) a kNN-LM architecture [20], where the output distribution is computed as a mixture of the distribution of the model and the retrieved documents.

![[Pasted image 20250505192215.png]]